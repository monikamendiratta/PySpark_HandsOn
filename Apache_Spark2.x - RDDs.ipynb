{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\" style=\"background:#2C3E50;color:white\">Basic Transformations and Actions - map, flatMap, reduce and more </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### command to launch pyspark explained below\n",
    "- in yarn mode\n",
    "- multi-tenant environment hence passing port number so that there is no conflict with other users\n",
    "- num-executors set to 2\n",
    "- disable spark dynamic allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --master yarn \\\n",
    " --conf spark.ui.port=21117 \\\n",
    " --num-executors 2 \\\n",
    " --conf spark.dynamicAlloction.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p style=\"background :#AED6F1\"><b>eg 1 - Sum of even numbers converting a collection into RDD</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> list(range(1, 10))\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    ">>> l = list(range(1, 10))\n",
    ">>> type(l)\n",
    "<type 'list'>\n",
    ">>> l = list(range(1, 100001))\n",
    ">>> len(l)\n",
    "100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting l into RDD using parallelize function to sum up even numbers\n",
    ">>> lRDD = sc.parallelize(l)\n",
    ">>> type(lRDD)\n",
    "<class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b>transformations take 1 RDD as input and they generate another RDD as output. they do not trigger the execution, it will update the DAG (Directed Acyclic Graph) associated with the variable of type RDD.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action on RDD\n",
    ">>> lRDD.count()\n",
    "[Stage 0:>                                                          (0 + 0) / 2]20/06/11 02:07:50 WARN TaskSetManager: Stage 0 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "100000            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b>when we perform an action, it will return the values to the driver program and also it triggers the execution of DAG as soon as it is performed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lRDD.first()\n",
    "20/06/11 02:08:17 WARN TaskSetManager: Stage 1 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "1\n",
    ">>> lRDD.take(10)\n",
    "20/06/11 02:08:30 WARN TaskSetManager: Stage 2 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering even numbers from lRDD\n",
    ">>> lEven = lRDD.filter(lambda x: x % 2 == 0)\n",
    ">>> type(lEven)\n",
    "<class 'pyspark.rdd.PipelinedRDD'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lEven.count()\n",
    "[Stage 3:>                                                          (0 + 0) / 2]20/06/11 02:14:17 WARN TaskSetManager: Stage 3 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "50000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce is an action\n",
    "#reduce returns only 1 value\n",
    ">>> lEven.reduce(lambda x, y: x + y)\n",
    "[Stage 4:>                                                          (0 + 0) / 2]20/06/11 02:17:43 WARN TaskSetManager: Stage 4 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "2500050000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lEven.reduce(lambda x, y: x if x < y else y)\n",
    "20/06/11 02:18:21 WARN TaskSetManager: Stage 5 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lEven.reduce(lambda x, y: x if x > y else y)\n",
    "20/06/11 02:18:36 WARN TaskSetManager: Stage 6 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative to above lambda func approach\n",
    "#package operator in python which has func add\n",
    "\n",
    ">>> from operator import add\n",
    ">>> lEven.reduce(add)\n",
    "20/06/11 02:19:44 WARN TaskSetManager: Stage 7 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
    "2500050000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><p style=\"background:#FFA07A;color:red;border:solid\"><b>NOTE :DO NOT USE BELOW COMMAND IN CERTIFICATION</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lRDD.collect()\n",
    "# be extermely careful in using this because it converts\n",
    "# entire RDD into collection, say 10gb data is there in RDD\n",
    "# whole data is converted into a collection of type list\n",
    "# and you run into out of memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p style=\"background :#AED6F1\"><b>eg 2 - Word Count Program </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> 1) flatMap -</b> convert a single record into multiple records based upon the logic. Number of records in output RDD will be greater than in input RDD.<br>\n",
    "<b> 2) map -</b> apply the transformation on individual records resulting in changed values. Number of records in input and output RDDs will be same.<br>\n",
    "<b> 3) reduceByKey -</b> generate aggregated result by processing data in input RDD. typically returns 1 value per key irrespective of the number of records in input RDD. reduceByKey is a transformation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>Problem Statment -</b>\n",
    "For uniques word in an input file we need to get how many times it is repeated.\n",
    "<br><b>Design -</b>\n",
    "<br>- Break each line into words using <b>flatMap</b>. <br>flatMap takes lambda function as argument for which we need to pass logic to break down input record into an array and flatMap's inbuilt logic will return each element in array as record.\n",
    "<br>- After we break each line into word, we need to convert them into tuples using <b>map</b>, with word as key and 1 as it's value.\n",
    "<br>- Paired RDD (o/p of above map) can now be passed to <b>reduceByKey</b> and get count of each word.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>eg word count solution using list collection</b>\n",
    "<br>just for prototyping</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg using list collection\n",
    ">>> l = [\"Hello how are you?\", \"you are welcome\", \"welcome to xxxxxx\"]\n",
    ">>> l\n",
    "['Hello how are you?', 'you are welcome', 'welcome to xxxxxx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> l[0]\n",
    "'Hello how are you?'\n",
    ">>> len(l)\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lRDD = sc.parallelize(l)\n",
    ">>> lRDD.count()\n",
    "3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> help(lRDD.flatMap)\n",
    "\n",
    ">>> s = l[0]\n",
    ">>> s\n",
    "'Hello how are you?'\n",
    ">>> s.split()\n",
    "['Hello', 'how', 'are', 'you?']\n",
    ">>> s.split(\" \")\n",
    "['Hello', 'how', 'are', 'you?']\n",
    ">>> s\n",
    "'Hello how are you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lFlatMap = lRDD.flatMap(lambda s: s.split(\" \"))\n",
    ">>> lFlatMap.count()\n",
    "10                                                                              \n",
    ">>> for i in lFlatMap.collect(): print(i)\n",
    "... \n",
    "Hello\n",
    "how\n",
    "are\n",
    "you?\n",
    "you\n",
    "are\n",
    "welcome\n",
    "welcome\n",
    "to\n",
    "xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lMap = lFlatMap.map(lambda s: (s, 1))\n",
    ">>> lMap.count()\n",
    "10                                                                              \n",
    ">>> for i in lMap.collect(): print(i)\n",
    "... \n",
    "('Hello', 1)                                                                    \n",
    "('how', 1)\n",
    "('are', 1)\n",
    "('you?', 1)\n",
    "('you', 1)\n",
    "('are', 1)\n",
    "('welcome', 1)\n",
    "('welcome', 1)\n",
    "('to', 1)\n",
    "('xxxxxx', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lMap.reduceByKey\n",
    "<bound method PipelinedRDD.reduceByKey of PythonRDD[7] at collect at <stdin>:1>\n",
    ">>> wc = lMap.reduceByKey(lambda x, y: x + y)\n",
    ">>> wc.count()\n",
    "8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in wc.collect(): print(i)\n",
    "... \n",
    "('you?', 1)\n",
    "('you', 1)\n",
    "('xxxxxx', 1)\n",
    "('to', 1)\n",
    "('Hello', 1)\n",
    "('welcome', 2)\n",
    "('are', 2)\n",
    "('how', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Solution to Word Count problem using transformations and action - reduceByKey</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lines = sc.textFile(\"/public/randomtextwriter/part-m-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> type(lines)\n",
    "<class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lines.count()\n",
    "[Stage 9:>                                                          (0 + 1) / 9]\n",
    "26421                                                                           \n",
    ">>> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> words = lines.flatMap(lambda s: s.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> wordTuples = words.map(lambda w: (w, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for t in wordTuples.take(10): print(t)\n",
    "... \n",
    "(u'SEQ\\x06\\x19org.apache.hadoop.io.Text\\x19org.apache.hadoop.io.Text\\x00\\x00\\x00\\x00\\x00\\x00\\ufffdg\\x05\\x081\\ufffd\\ufffdJ$\\ufffd\\u05d2\\x1ad\\ufffd\\x08\\x00\\x00\\x02\\ufffd\\x00\\x00\\x00XWpterostigma', 1)\n",
    "(u'steprelationship', 1)\n",
    "(u'pleasurehood', 1)\n",
    "(u'abusiveness', 1)\n",
    "(u'seelful', 1)\n",
    "(u'unstipulated', 1)\n",
    "(u'winterproof', 1)\n",
    "(u'\\ufffd\\x02gmericarp', 1)\n",
    "(u'pentosuria', 1)\n",
    "(u'airfreighter', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from operator import add\n",
    ">>> wordCount = wordTuples.reduceByKey(add)\n",
    ">>> wordCount.count()\n",
    "1588631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Orders -- just example</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders = sc.textFile(\"/public/retail_db/orders\")\n",
    "\n",
    ">>> orders.first()\n",
    "u'1,2013-07-25 00:00:00.0,11599,CLOSED'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in orders.take(10): print(i)\n",
    "... \n",
    "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
    "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
    "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
    "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
    "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
    "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
    "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
    "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
    "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
    "10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> o = \"1,2013-07-25 00:00:00.0,11599,CLOSED\"\n",
    ">>> o.split(\",\")\n",
    "['1', '2013-07-25 00:00:00.0', '11599', 'CLOSED']\n",
    ">>> o.split(\",\")[3]\n",
    "'CLOSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.map(lambda o: o.split(\",\")[3]).take(10)\n",
    "[u'CLOSED', u'PENDING_PAYMENT', u'COMPLETE', u'CLOSED', u'COMPLETE', u'COMPLETE', u'COMPLETE', u'PROCESSING', u'PENDING_PAYMENT', u'PENDING_PAYMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.map(lambda o: int(o.split(\",\")[1][:4])).take(10)\n",
    "[2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\" style=\"background:#2C3E50;color:white\">Basic Transformations and Actions - Shuffling (reduceByKey, groupByKey, aggregateByKey) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>groupByKey & reduceByKey </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#AED6F1\"><b>eg 1 - Compute Revenue for each order id </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert \n",
    "**(2, 199.99), (2, 250.0), (2, 129.99) -> [(2, [199.99, 250.0, 129.99])]**\n",
    "that is input RDD of paired tuples to output RDD which has 1 record having a tuple whose first element is a unique key and the second element is a collection.<br>\n",
    "**why we need to do this ?**\n",
    "=> either to add the second elements' of the collection or to sort them or to rank them.\n",
    "groupByKey facilitates to group the data like this and after that necessary logic can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems = sc.textFile(\"/public/retail_db/order_items/part-00000\")\n",
    ">>> type(orderItems)\n",
    "<class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems.take(10)\n",
    "[u'1,1,957,1,299.98,299.98', u'2,2,1073,1,199.99,199.99', u'3,2,502,5,250.0,50.0', u'4,2,403,1,129.99,129.99', u'5,4,897,2,49.98,24.99', u'6,4,365,5,299.95,59.99', u'7,4,502,3,150.0,50.0', u'8,4,1014,4,199.92,49.98', u'9,5,957,1,299.98,299.98', u'10,5,365,5,299.95,59.99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better readability\n",
    "\n",
    ">>> for oi in orderItems.take(10): print(oi)\n",
    "... \n",
    "1,1,957,1,299.98,299.98\n",
    "2,2,1073,1,199.99,199.99\n",
    "3,2,502,5,250.0,50.0\n",
    "4,2,403,1,129.99,129.99\n",
    "5,4,897,2,49.98,24.99\n",
    "6,4,365,5,299.95,59.99\n",
    "7,4,502,3,150.0,50.0\n",
    "8,4,1014,4,199.92,49.98\n",
    "9,5,957,1,299.98,299.98\n",
    "10,5,365,5,299.95,59.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for visualizing what we need as input RDD \n",
    "# A tuple with order_id and it's subtotal\n",
    "\n",
    ">>> oi = \"2,2,1073,1,199.99,199.99\"\n",
    ">>> oi.split(\",\")[1]\n",
    "'2'\n",
    ">>> (int(oi.split(\",\")[1]),float(oi.split(\",\")[4]))\n",
    "(2, 199.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create a RDD of mapped paired tuples of order_id and it's subtotal\n",
    "\n",
    ">>> orderItemsMap = orderItems.map(lambda oi: (int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "\n",
    "# just for viewing what the above mapped RDD holds\n",
    "\n",
    ">>> for i in orderItemsMap.take(10): print(i)\n",
    "... \n",
    "(1, 299.98)                                                                     \n",
    "(2, 199.99)\n",
    "(2, 250.0)\n",
    "(2, 129.99)\n",
    "(4, 49.98)\n",
    "(4, 299.95)\n",
    "(4, 150.0)\n",
    "(4, 199.92)\n",
    "(5, 299.98)\n",
    "(5, 299.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as orderItems count, that is total number of records in the data file\n",
    "\n",
    ">>> orderItemsMap.count()\n",
    "172198 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>using groupByKey()</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrongly performed groupByKey on initial input RDD \n",
    "# should be performed on orderItemsMap instead\n",
    "\n",
    ">>> orderItemsMapGBK = orderItems.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrongly counted number of records in it\n",
    "\n",
    ">>> orderItemsMapGBK.count()\n",
    "# gives error\n",
    "# ValueError: too many values to unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct groupByKey applied on RDD of mapped paired tuples\n",
    "\n",
    ">>> orderItemsMapGBK = orderItemsMap.groupByKey()\n",
    ">>> orderItemsMapGBK.count()\n",
    "57431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in orderItemsMapGBK: print(i)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "TypeError: 'PipelinedRDD' object is not iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing what grouped by key RDD holds\n",
    "# (order_id, iterable object <collection>)\n",
    "\n",
    ">>> for i in orderItemsMapGBK.take(10): print(i)\n",
    "... \n",
    "(2, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5250>)\n",
    "(4, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5410>)\n",
    "(8, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5450>)\n",
    "(10, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5490>)\n",
    "(12, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f54d0>)\n",
    "(14, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5510>)\n",
    "(16, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5550>)\n",
    "(18, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5590>)\n",
    "(20, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f55d0>)\n",
    "(24, <pyspark.resultiterable.ResultIterable object at 0x7fcd819f5610>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we use groupByKey for aggregating we again need \n",
    "# to create a map for performing aggregation function\n",
    "# on the grouped by key RDD\n",
    "\n",
    ">>> orderRevenue = orderItemsMapGBK.map(lambda o: (o[0], sum(o[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after above mapping we get the same number of records \n",
    "# as in grouped by key RDD, that is what map function\n",
    "# does -> transforms but returns same number of records\n",
    "\n",
    ">>> orderRevenue.count()\n",
    "57431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing what ouput RDD holds\n",
    "\n",
    ">>> for rev in orderRevenue.take(10): print(rev)\n",
    "... \n",
    "(2, 579.98)                                                                     \n",
    "(4, 699.85)\n",
    "(8, 729.8399999999999)\n",
    "(10, 651.9200000000001)\n",
    "(12, 1299.8700000000001)\n",
    "(14, 549.94)\n",
    "(16, 419.93)\n",
    "(18, 449.96000000000004)\n",
    "(20, 879.8599999999999)\n",
    "(24, 829.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>using reduceByKey()</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the aggregation by reduceByKey significantly\n",
    "# improves the throughput as it doesn't require us to \n",
    "# map for the second time and also does shuffling internally\n",
    "\n",
    "# hence for such scenarios if the problem is to be \n",
    "# solved using core APIs of transformations and actions\n",
    "#reduceByKey should be preferred\n",
    "\n",
    ">>> orderRevenueRBK = orderItemsMap.reduceByKey(lambda x, y: x + y)\n",
    ">>> for i in orderRevenueRBK.take(10): print(i)\n",
    "... \n",
    "(2, 579.98)                                                                     \n",
    "(4, 699.85)\n",
    "(8, 729.8399999999999)\n",
    "(10, 651.9200000000001)\n",
    "(12, 1299.8700000000001)\n",
    "(14, 549.94)\n",
    "(16, 419.93)\n",
    "(18, 449.96000000000004)\n",
    "(20, 879.8599999999999)\n",
    "(24, 829.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b> Shuffling includes partitioning, grouping, optionally computing intermediate values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\">Since dynamicAllocation is disabled and num executors is set to 2, the file would have been processed by 2 executors hence in 2 tasks but we wanted to have 4 partitions hence we manually repartitioned it into 4 partitions by using below command.</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems = sc.textFile(\"/public/retail_db/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems.repartition(4).saveAsTextFile(\"/user/monahadoop/pyspark/orderItemsPartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/pyspark/orderItemsPartitioned\n",
    "Found 5 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-06-14 22:21 /user/monahadoop/pyspark/orderItemsPartitioned/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs    1351889 2020-06-14 22:21 /user/monahadoop/pyspark/orderItemsPartitioned/part-00000\n",
    "-rw-r--r--   2 monahadoop hdfs    1352498 2020-06-14 22:21 /user/monahadoop/pyspark/orderItemsPartitioned/part-00001\n",
    "-rw-r--r--   2 monahadoop hdfs    1351882 2020-06-14 22:21 /user/monahadoop/pyspark/orderItemsPartitioned/part-00002\n",
    "-rw-r--r--   2 monahadoop hdfs    1352611 2020-06-14 22:21 /user/monahadoop/pyspark/orderItemsPartitioned/part-00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\">To further see that we are indeed using 4 partitions-></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems = sc.textFile(\"/user/monahadoop/pyspark/orderItemsPartitioned\")\n",
    ">>> orderItems.count()\n",
    "172198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\">By default the log level is other, to see logs and verify that we are now using 4 partitions we can set log level to INFO.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItemsMap = orderItems.map(lambda oi: (int(oi.split(\",\")[1]), float(oi.split(\",\")[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItemsGBK = orderItemsMap.groupByKey(3)\n",
    "# 3 is optinal argument of number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\" style=\"background:#2C3E50;color:white\">Basic Transformations and Actions - filter, joins and sortByKey </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b> As part of lambda functions which are passed to APIs such as map, flatMap, filter etc. - the logic should be pure python.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>eg 1 - Compute Daily Revenue by using filter, joins and sortByKey </b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders = sc.textFile(\"/public/retail_db/orders\")\n",
    "\n",
    ">>> type(orders)\n",
    "<class 'pyspark.rdd.RDD'>\n",
    "\n",
    ">>> orders.take(10)\n",
    "['1,2013-07-25 00:00:00.0,11599,CLOSED', '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT', '3,2013-07-25 00:00:00.0,12111,COMPLETE', '4,2013-07-25 00:00:00.0,8827,CLOSED', '5,2013-07-25 00:00:00.0,11318,COMPLETE', '6,2013-07-25 00:00:00.0,7130,COMPLETE', '7,2013-07-25 00:00:00.0,4530,COMPLETE', '8,2013-07-25 00:00:00.0,2911,PROCESSING', '9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT', '10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for o in orders.take(10): print(o)\n",
    "... \n",
    "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
    "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
    "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
    "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
    "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
    "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
    "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
    "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
    "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
    "10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersFiltered = orders.filter(lambda o: o.split(\",\")[3] in('CLOSED', 'COMPLETE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> type(ordersFiltered)\n",
    "<class 'pyspark.rdd.PipelinedRDD'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersFiltered.count()\n",
    "30455  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItems = sc.textFile(\"/public/retail_db/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for oi in orderItems.takeSample(True, 10): print(oi)\n",
    "... \n",
    "62021,24759,191,1,99.99,99.99                                                   \n",
    "49920,19967,403,1,129.99,129.99\n",
    "82937,33177,365,3,179.97,59.99\n",
    "63944,25522,823,1,51.99,51.99\n",
    "101697,40757,502,5,250.0,50.0\n",
    "110762,44368,403,1,129.99,129.99\n",
    "12278,4910,957,1,299.98,299.98\n",
    "40742,16321,502,5,250.0,50.0\n",
    "72244,28869,191,2,199.98,99.99\n",
    "116843,46732,957,1,299.98,299.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersMap = ordersFiltered.map(lambda o: (int(o.split(\",\")[0]), o.split(\",\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersMap.take(10): print(i)\n",
    "... \n",
    "(1, '2013-07-25 00:00:00.0')\n",
    "(3, '2013-07-25 00:00:00.0')\n",
    "(4, '2013-07-25 00:00:00.0')\n",
    "(5, '2013-07-25 00:00:00.0')\n",
    "(6, '2013-07-25 00:00:00.0')\n",
    "(7, '2013-07-25 00:00:00.0')\n",
    "(12, '2013-07-25 00:00:00.0')\n",
    "(15, '2013-07-25 00:00:00.0')\n",
    "(17, '2013-07-25 00:00:00.0')\n",
    "(18, '2013-07-25 00:00:00.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItemsMap = orderItems.map(lambda oi: (int(oi.split(\",\")[1]), float(oi.split(\",\")[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in orderItemsMap.take(10): print(i)\n",
    "... \n",
    "(1, 299.98)                                                                     \n",
    "(2, 199.99)\n",
    "(2, 250.0)\n",
    "(2, 129.99)\n",
    "(4, 49.98)\n",
    "(4, 299.95)\n",
    "(4, 150.0)\n",
    "(4, 199.92)\n",
    "(5, 299.98)\n",
    "(5, 299.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersJoin = ordersMap.join(orderItemsMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersJoin.take(10): print(i)\n",
    "... \n",
    "(4, ('2013-07-25 00:00:00.0', 49.98))                                           \n",
    "(4, ('2013-07-25 00:00:00.0', 299.95))\n",
    "(4, ('2013-07-25 00:00:00.0', 150.0))\n",
    "(4, ('2013-07-25 00:00:00.0', 199.92))\n",
    "(12, ('2013-07-25 00:00:00.0', 299.98))\n",
    "(12, ('2013-07-25 00:00:00.0', 100.0))\n",
    "(12, ('2013-07-25 00:00:00.0', 149.94))\n",
    "(12, ('2013-07-25 00:00:00.0', 499.95))\n",
    "(12, ('2013-07-25 00:00:00.0', 250.0))\n",
    "(24, ('2013-07-25 00:00:00.0', 129.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersJoin.take(10): print(i)\n",
    "... \n",
    "(35228, ('2014-02-27 00:00:00.0', 249.9))\n",
    "(35228, ('2014-02-27 00:00:00.0', 399.98))\n",
    "(35232, ('2014-02-27 00:00:00.0', 100.0))\n",
    "(35248, ('2014-02-27 00:00:00.0', 199.92))\n",
    "(35264, ('2014-02-27 00:00:00.0', 200.0))\n",
    "(35264, ('2014-02-27 00:00:00.0', 239.96))\n",
    "(35272, ('2014-02-27 00:00:00.0', 30.0))\n",
    "(35272, ('2014-02-27 00:00:00.0', 129.99))\n",
    "(35272, ('2014-02-27 00:00:00.0', 59.99))\n",
    "(35272, ('2014-02-27 00:00:00.0', 299.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersJoinMap = ordersJoin.map(lambda o: o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersJoinMap.take(10): print(i)\n",
    "... \n",
    "('2013-07-25 00:00:00.0', 49.98)                                                \n",
    "('2013-07-25 00:00:00.0', 299.95)\n",
    "('2013-07-25 00:00:00.0', 150.0)\n",
    "('2013-07-25 00:00:00.0', 199.92)\n",
    "('2013-07-25 00:00:00.0', 299.98)\n",
    "('2013-07-25 00:00:00.0', 100.0)\n",
    "('2013-07-25 00:00:00.0', 149.94)\n",
    "('2013-07-25 00:00:00.0', 499.95)\n",
    "('2013-07-25 00:00:00.0', 250.0)\n",
    "('2013-07-25 00:00:00.0', 129.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersJoinMap.count()\n",
    "75408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> DailyRevenue = ordersJoinMap.reduceByKey(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other way to add\n",
    "\n",
    ">>> from operator import add\n",
    ">>> DailyRevenue = ordersJoinMap.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> DailyRevenue.count()\n",
    "[Stage 39:===========================================>              (3 + 1) / 4]\n",
    "364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in DailyRevenue.take(10): print(i)\n",
    "... \n",
    "('2014-03-03 00:00:00.0', 52553.409999999974)\n",
    "('2014-03-05 00:00:00.0', 43432.309999999976)\n",
    "('2014-03-06 00:00:00.0', 42483.269999999975)\n",
    "('2014-03-07 00:00:00.0', 37843.39999999998)\n",
    "('2014-03-12 00:00:00.0', 54095.61999999995)\n",
    "('2014-03-18 00:00:00.0', 45921.39999999997)\n",
    "('2014-03-25 00:00:00.0', 27971.80999999999)\n",
    "('2014-03-26 00:00:00.0', 57003.34999999995)\n",
    "('2014-03-27 00:00:00.0', 28715.84999999999)\n",
    "('2014-04-04 00:00:00.0', 44999.059999999976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> DailyRevenue = ordersJoinMap.reduceByKey(lambda x,y: round((x + y), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in DailyRevenue.take(10): print(i)\n",
    "... \n",
    "('2014-03-03 00:00:00.0', 52553.41)                                             \n",
    "('2014-03-05 00:00:00.0', 43432.31)\n",
    "('2014-03-06 00:00:00.0', 42483.27)\n",
    "('2014-03-07 00:00:00.0', 37843.4)\n",
    "('2014-03-12 00:00:00.0', 54095.62)\n",
    "('2014-03-18 00:00:00.0', 45921.4)\n",
    "('2014-03-25 00:00:00.0', 27971.81)\n",
    "('2014-03-26 00:00:00.0', 57003.35)\n",
    "('2014-03-27 00:00:00.0', 28715.85)\n",
    "('2014-04-04 00:00:00.0', 44999.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting daily_revenue in asc order of date\n",
    "\n",
    ">>> DailyRevenueSorted = DailyRevenue.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in DailyRevenueSorted.take(10): print(i)\n",
    "... \n",
    "('2013-07-25 00:00:00.0', 31547.23)\n",
    "('2013-07-26 00:00:00.0', 54713.23)\n",
    "('2013-07-27 00:00:00.0', 48411.48)\n",
    "('2013-07-28 00:00:00.0', 35672.03)\n",
    "('2013-07-29 00:00:00.0', 54579.7)\n",
    "('2013-07-30 00:00:00.0', 49329.29)\n",
    "('2013-07-31 00:00:00.0', 59212.49)\n",
    "('2013-08-01 00:00:00.0', 49160.08)\n",
    "('2013-08-02 00:00:00.0', 50688.58)\n",
    "('2013-08-03 00:00:00.0', 43416.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting daily_revenue in desc order of date\n",
    "\n",
    ">>> DailyRevenueSorted = DailyRevenue.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in DailyRevenueSorted.take(10): print(i)\n",
    "... \n",
    "('2014-07-24 00:00:00.0', 50885.19)\n",
    "('2014-07-23 00:00:00.0', 38795.23)\n",
    "('2014-07-22 00:00:00.0', 36717.24)\n",
    "('2014-07-21 00:00:00.0', 51427.7)\n",
    "('2014-07-20 00:00:00.0', 60047.45)\n",
    "('2014-07-19 00:00:00.0', 38420.99)\n",
    "('2014-07-18 00:00:00.0', 43856.6)\n",
    "('2014-07-17 00:00:00.0', 36384.77)\n",
    "('2014-07-16 00:00:00.0', 43011.92)\n",
    "('2014-07-15 00:00:00.0', 53480.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> DailyRevenueSorted = DailyRevenue.sortByKey(True)\n",
    "\n",
    ">>> for i in DailyRevenueSorted.take(10): print(i)                              \n",
    "... \n",
    "('2013-07-25 00:00:00.0', 31547.23)\n",
    "('2013-07-26 00:00:00.0', 54713.23)\n",
    "('2013-07-27 00:00:00.0', 48411.48)\n",
    "('2013-07-28 00:00:00.0', 35672.03)\n",
    "('2013-07-29 00:00:00.0', 54579.7)\n",
    "('2013-07-30 00:00:00.0', 49329.29)\n",
    "('2013-07-31 00:00:00.0', 59212.49)\n",
    "('2013-08-01 00:00:00.0', 49160.08)\n",
    "('2013-08-02 00:00:00.0', 50688.58)\n",
    "('2013-08-03 00:00:00.0', 43416.74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b> Transforming into string format in which this data is to be saved into file. Comma Separated Values instead of tuples.<br><code>lambda o: o[0] + \",\" + str(o[1])</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # transforming into string format in which this data is to be saved into file\n",
    "\n",
    ">>> for i in DailyRevenueSorted.map(lambda o: o[0] + \",\" + str(o[1])).take(10):\n",
    "...     print(i)\n",
    "... \n",
    "2013-07-25 00:00:00.0,31547.23                                                  \n",
    "2013-07-26 00:00:00.0,54713.23\n",
    "2013-07-27 00:00:00.0,48411.48\n",
    "2013-07-28 00:00:00.0,35672.03\n",
    "2013-07-29 00:00:00.0,54579.7\n",
    "2013-07-30 00:00:00.0,49329.29\n",
    "2013-07-31 00:00:00.0,59212.49\n",
    "2013-08-01 00:00:00.0,49160.08\n",
    "2013-08-02 00:00:00.0,50688.58\n",
    "2013-08-03 00:00:00.0,43416.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> DailyRevenueSorted.map(lambda o: o[0] + \",\" + str(o[1])).saveAsTextFile(\"/user/monahadoop/pyspark/daily_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/pyspark/daily_revenue\n",
    "Found 5 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-06-16 03:54 /user/monahadoop/pyspark/daily_revenue/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs       3086 2020-06-16 03:54 /user/monahadoop/pyspark/daily_revenue/part-00000\n",
    "-rw-r--r--   2 monahadoop hdfs       1667 2020-06-16 03:54 /user/monahadoop/pyspark/daily_revenue/part-00001\n",
    "-rw-r--r--   2 monahadoop hdfs       3921 2020-06-16 03:54 /user/monahadoop/pyspark/daily_revenue/part-00002\n",
    "-rw-r--r--   2 monahadoop hdfs       2565 2020-06-16 03:54 /user/monahadoop/pyspark/daily_revenue/part-00003\n",
    "\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -cat /user/monahadoop/pyspark/daily_revenue/part-00003\n",
    "2014-05-03 00:00:00.0,49541.22\n",
    "2014-05-04 00:00:00.0,31853.8\n",
    "2014-05-05 00:00:00.0,37446.95\n",
    "2014-05-06 00:00:00.0,60351.47\n",
    "2014-05-07 00:00:00.0,45749.36\n",
    "2014-05-08 00:00:00.0,23799.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><br></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>eg2  - left and right outer joins </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b> To apply join between to RDDs we need to first convert them into (K,V) pairs or paired tuples.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE :</b> To convert orders and orderItems RDDs to paired tuples -> lambda function would contain (order_id, whole RDD) as below:<br><code>lambda o: (int(o.split(\",\")[0]), o)</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #joining orders and order_items whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersMap1 = orders.map(lambda o: (int(o.split(\",\")[0]), o))\n",
    "\n",
    ">>> for i in ordersMap1.take(10): print(i)\n",
    "... \n",
    "(1, '1,2013-07-25 00:00:00.0,11599,CLOSED')                                     \n",
    "(2, '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT')\n",
    "(3, '3,2013-07-25 00:00:00.0,12111,COMPLETE')\n",
    "(4, '4,2013-07-25 00:00:00.0,8827,CLOSED')\n",
    "(5, '5,2013-07-25 00:00:00.0,11318,COMPLETE')\n",
    "(6, '6,2013-07-25 00:00:00.0,7130,COMPLETE')\n",
    "(7, '7,2013-07-25 00:00:00.0,4530,COMPLETE')\n",
    "(8, '8,2013-07-25 00:00:00.0,2911,PROCESSING')\n",
    "(9, '9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT')\n",
    "(10, '10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orderItemsMap1 = orderItems.map(lambda oi: (int(oi.split(\",\")[1]), oi))\n",
    "\n",
    ">>> for i in orderItemsMap1.take(10): print(i)\n",
    "... \n",
    "(1, '1,1,957,1,299.98,299.98')                                                  \n",
    "(2, '2,2,1073,1,199.99,199.99')\n",
    "(2, '3,2,502,5,250.0,50.0')\n",
    "(2, '4,2,403,1,129.99,129.99')\n",
    "(4, '5,4,897,2,49.98,24.99')\n",
    "(4, '6,4,365,5,299.95,59.99')\n",
    "(4, '7,4,502,3,150.0,50.0')\n",
    "(4, '8,4,1014,4,199.92,49.98')\n",
    "(5, '9,5,957,1,299.98,299.98')\n",
    "(5, '10,5,365,5,299.95,59.99')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for extracting distinct records\n",
    "\n",
    ">>> ordersMap1.join(orderItemsMap1).map(lambda o: o[0]).distinct().count()\n",
    "57431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Left Outer Join</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left outer join to find orders with no order_items - from business point of view it cannot and should not be\n",
    "# all data from left table and matching + Null from right table\n",
    "\n",
    ">>> ordersLOJoind = ordersMap1.leftOuterJoin(orderItemsMap1)\n",
    "\n",
    ">>> for i in ordersLOJoind.take(10): print(i)\n",
    "... \n",
    "(4, ('4,2013-07-25 00:00:00.0,8827,CLOSED', '5,4,897,2,49.98,24.99'))           \n",
    "(4, ('4,2013-07-25 00:00:00.0,8827,CLOSED', '6,4,365,5,299.95,59.99'))\n",
    "(4, ('4,2013-07-25 00:00:00.0,8827,CLOSED', '7,4,502,3,150.0,50.0'))\n",
    "(4, ('4,2013-07-25 00:00:00.0,8827,CLOSED', '8,4,1014,4,199.92,49.98'))\n",
    "(8, ('8,2013-07-25 00:00:00.0,2911,PROCESSING', '17,8,365,3,179.97,59.99'))\n",
    "(8, ('8,2013-07-25 00:00:00.0,2911,PROCESSING', '18,8,365,5,299.95,59.99'))\n",
    "(8, ('8,2013-07-25 00:00:00.0,2911,PROCESSING', '19,8,1014,4,199.92,49.98'))\n",
    "(8, ('8,2013-07-25 00:00:00.0,2911,PROCESSING', '20,8,502,1,50.0,50.0'))\n",
    "(12, ('12,2013-07-25 00:00:00.0,1837,CLOSED', '34,12,957,1,299.98,299.98'))\n",
    "(12, ('12,2013-07-25 00:00:00.0,1837,CLOSED', '35,12,134,4,100.0,25.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordersWithNoOrderItems contains orders which have Null in order items\n",
    "\n",
    ">>> ordersWithNoOrderItems = ordersLOJoind.filter(lambda o: o[1][1] == None)\n",
    "\n",
    ">>> ordersWithNoOrderItems.count()\n",
    "11452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersWithNoOrderItems.take(100): print(i)\n",
    "... \n",
    "(34568, ('34568,2014-02-23 00:00:00.0,1271,COMPLETE', None))\n",
    "(34572, ('34572,2014-02-23 00:00:00.0,8135,PENDING', None))\n",
    "(34580, ('34580,2014-02-23 00:00:00.0,6540,COMPLETE', None))\n",
    "(34688, ('34688,2014-02-24 00:00:00.0,8033,SUSPECTED_FRAUD', None))\n",
    "(34704, ('34704,2014-02-24 00:00:00.0,2858,COMPLETE', None))\n",
    "(34812, ('34812,2014-02-24 00:00:00.0,8435,COMPLETE', None))\n",
    "(34872, ('34872,2014-02-25 00:00:00.0,9176,PENDING_PAYMENT', None))\n",
    "(34896, ('34896,2014-02-25 00:00:00.0,10749,PENDING', None))\n",
    "(34920, ('34920,2014-02-25 00:00:00.0,10997,COMPLETE', None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE -> SAMPLE DATA TO CREATE LOGIC\n",
    "\n",
    ">>> o = (36888, ('36888,2014-03-08 00:00:00.0,5248,PENDING', None))\n",
    ">>> type(o)\n",
    "<class 'tuple'>\n",
    ">>> o[0]\n",
    "36888\n",
    ">>> o[1]\n",
    "('36888,2014-03-08 00:00:00.0,5248,PENDING', None)\n",
    ">>> o[1][0]\n",
    "'36888,2014-03-08 00:00:00.0,5248,PENDING'\n",
    ">>> o[1][1]\n",
    ">>> o[1][1] == None\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Right Outer Join</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersROJoin = orderItemsMap1.rightOuterJoin(ordersMap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersROJoin.take(100): print(i)\n",
    "... \n",
    "(4, ('5,4,897,2,49.98,24.99', '4,2013-07-25 00:00:00.0,8827,CLOSED'))           \n",
    "(4, ('6,4,365,5,299.95,59.99', '4,2013-07-25 00:00:00.0,8827,CLOSED'))\n",
    "(4, ('7,4,502,3,150.0,50.0', '4,2013-07-25 00:00:00.0,8827,CLOSED'))\n",
    "(4, ('8,4,1014,4,199.92,49.98', '4,2013-07-25 00:00:00.0,8827,CLOSED'))\n",
    "(8, ('17,8,365,3,179.97,59.99', '8,2013-07-25 00:00:00.0,2911,PROCESSING'))\n",
    "(8, ('18,8,365,5,299.95,59.99', '8,2013-07-25 00:00:00.0,2911,PROCESSING'))\n",
    "(8, ('19,8,1014,4,199.92,49.98', '8,2013-07-25 00:00:00.0,2911,PROCESSING'))\n",
    "(8, ('20,8,502,1,50.0,50.0', '8,2013-07-25 00:00:00.0,2911,PROCESSING'))\n",
    "(12, ('34,12,957,1,299.98,299.98', '12,2013-07-25 00:00:00.0,1837,CLOSED'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersROJoin.count()\n",
    "183650\n",
    ">>> ordersLOJoind.count()\n",
    "183650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersWithNoOrderItems = ordersROJoin.filter(lambda o: (o[1][0] == None))\n",
    "\n",
    ">>> ordersWithNoOrderItems.count()\n",
    "11452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in ordersWithNoOrderItems.take(100): print(i)\n",
    "... \n",
    "(34568, (None, '34568,2014-02-23 00:00:00.0,1271,COMPLETE'))\n",
    "(34572, (None, '34572,2014-02-23 00:00:00.0,8135,PENDING'))\n",
    "(34580, (None, '34580,2014-02-23 00:00:00.0,6540,COMPLETE'))\n",
    "(34688, (None, '34688,2014-02-24 00:00:00.0,8033,SUSPECTED_FRAUD'))\n",
    "(34704, (None, '34704,2014-02-24 00:00:00.0,2858,COMPLETE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><br></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>eg3  - composite sorting </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE : </b>Composite sorting means sorting data based on two keys' values. In the example below, data is sorted based on keys - order_id and subtotal. We need only 1 RDD i.e. orderItems.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in orderItems.take(10): print(i)\n",
    "... \n",
    "1,1,957,1,299.98,299.98                                                         \n",
    "2,2,1073,1,199.99,199.99\n",
    "3,2,502,5,250.0,50.0\n",
    "4,2,403,1,129.99,129.99\n",
    "5,4,897,2,49.98,24.99\n",
    "6,4,365,5,299.95,59.99\n",
    "7,4,502,3,150.0,50.0\n",
    "8,4,1014,4,199.92,49.98\n",
    "9,5,957,1,299.98,299.98\n",
    "10,5,365,5,299.95,59.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>step 1 : </b>Below creating a map of order_items, which results in nested paired tuple for each record.<br>As in, ((oi.order_id, oi. subtotal), orderItems)<br><code>lambda oi: ((int(oi.split(\",\")[1]), float(oi.split(\",\")[4])), oi)</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> oiMap = orderItems.map(lambda oi: ((int(oi.split(\",\")[1]), float(oi.split(\",\")[4])), oi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in oiMap.take(10): print(i)\n",
    "... \n",
    "((1, 299.98), '1,1,957,1,299.98,299.98')                                        \n",
    "((2, 199.99), '2,2,1073,1,199.99,199.99')\n",
    "((2, 250.0), '3,2,502,5,250.0,50.0')\n",
    "((2, 129.99), '4,2,403,1,129.99,129.99')\n",
    "((4, 49.98), '5,4,897,2,49.98,24.99')\n",
    "((4, 299.95), '6,4,365,5,299.95,59.99')\n",
    "((4, 150.0), '7,4,502,3,150.0,50.0')\n",
    "((4, 199.92), '8,4,1014,4,199.92,49.98')\n",
    "((5, 299.98), '9,5,957,1,299.98,299.98')\n",
    "((5, 299.95), '10,5,365,5,299.95,59.99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>step 2 : </b>Sort above data in ascending order of key. Key being (oi.order_id, oi. subtotal). sortByKey() has True as default argument. True for sorting in ascending order.<br><code>(int(oi.split(\",\")[1]), float(oi.split(\",\")[4])) -> sortByKey()</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in oiMap.sortByKey().take(10): print(i)\n",
    "... \n",
    "((1, 299.98), '1,1,957,1,299.98,299.98')                                        \n",
    "((2, 129.99), '4,2,403,1,129.99,129.99')\n",
    "((2, 199.99), '2,2,1073,1,199.99,199.99')\n",
    "((2, 250.0), '3,2,502,5,250.0,50.0')\n",
    "((4, 49.98), '5,4,897,2,49.98,24.99')\n",
    "((4, 150.0), '7,4,502,3,150.0,50.0')\n",
    "((4, 199.92), '8,4,1014,4,199.92,49.98')\n",
    "((4, 299.95), '6,4,365,5,299.95,59.99')\n",
    "((5, 99.96), '11,5,1014,2,99.96,49.98')\n",
    "((5, 129.99), '13,5,403,1,129.99,129.99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>step 3 : </b>Sort above data in descending order of key. Key being (oi.order_id, oi. subtotal). sortByKey(False). False for sorting in descending order.<br><code>(int(oi.split(\",\")[1]), float(oi.split(\",\")[4])) -> sortByKey(False)</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in oiMap.sortByKey(False).take(10): print(i)\n",
    "... \n",
    "((68883, 1999.99), '172197,68883,208,1,1999.99,1999.99')                        \n",
    "((68883, 150.0), '172198,68883,502,3,150.0,50.0')\n",
    "((68882, 59.99), '172195,68882,365,1,59.99,59.99')\n",
    "((68882, 50.0), '172196,68882,502,1,50.0,50.0')\n",
    "((68881, 129.99), '172194,68881,403,1,129.99,129.99')\n",
    "((68880, 250.0), '172190,68880,502,5,250.0,50.0')\n",
    "((68880, 249.9), '172192,68880,1014,5,249.9,49.98')\n",
    "((68880, 199.99), '172191,68880,1073,1,199.99,199.99')\n",
    "((68880, 149.94), '172189,68880,1014,3,149.94,49.98')\n",
    "((68880, 149.94), '172193,68880,1014,3,149.94,49.98')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>step 4 : </b> Now if we want to sort records in ascending order of order_id first and descending order of subtotal within, then using a trick <br>-> making subtotals as -ve, will arrange them in descending order of their values (but ascending order wrt -ve value) <br>-> hence the corresponding records will be arranged in order <br>-> order_id asc, subtotal desc.<br>Later we could discard the keys used for sorting and just display sorted records as output. <br><code>(int(oi.split(\",\")[1]), -float(oi.split(\",\")[4])) -> sortByKey(False)</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> oiMap = orderItems.map(lambda oi: ((int(oi.split(\",\")[1]), -float(oi.split(\",\")[4])), oi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for i in oiMap.sortByKey().take(10): print(i)\n",
    "... \n",
    "((1, -299.98), '1,1,957,1,299.98,299.98')                                       \n",
    "((2, -250.0), '3,2,502,5,250.0,50.0')\n",
    "((2, -199.99), '2,2,1073,1,199.99,199.99')\n",
    "((2, -129.99), '4,2,403,1,129.99,129.99')\n",
    "((4, -299.95), '6,4,365,5,299.95,59.99')\n",
    "((4, -199.92), '8,4,1014,4,199.92,49.98')\n",
    "((4, -150.0), '7,4,502,3,150.0,50.0')\n",
    "((4, -49.98), '5,4,897,2,49.98,24.99')\n",
    "((5, -299.98), '9,5,957,1,299.98,299.98')\n",
    "((5, -299.98), '12,5,957,1,299.98,299.98')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #once the data is sorted based on composite key in this case output above is sorted by \n",
    "... #order id ascending and subtotal descending, we can discard the key and display the sorted data \n",
    "... # as follows\n",
    "\n",
    ">>> for i in oiMap.sortByKey(True).map(lambda o: o[1]).take(10): print(i)\n",
    "... \n",
    "1,1,957,1,299.98,299.98                                                         \n",
    "3,2,502,5,250.0,50.0\n",
    "2,2,1073,1,199.99,199.99\n",
    "4,2,403,1,129.99,129.99\n",
    "6,4,365,5,299.95,59.99\n",
    "8,4,1014,4,199.92,49.98\n",
    "7,4,502,3,150.0,50.0\n",
    "5,4,897,2,49.98,24.99\n",
    "9,5,957,1,299.98,299.98\n",
    "12,5,957,1,299.98,299.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\" style=\"background:#2C3E50;color:white\">Accumulators and Broadcast Variables, Repartition and Coalesce 01</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>Accumulators and Broadcast variables</b></p> \n",
    "\n",
    "* are also known as Shared Variables.\n",
    "* Accumulators are primarily used as counters for sanity checks.\n",
    "* Broadcast variables are used for lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>Revenue per Product for a given month using Accumulators</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Problem Statement</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use HDFS APIs as part of applications built using pyspark.\n",
    "* We have to use orders, order_items and products data sets to compute revenue per product for a given month.\n",
    "* From  orders -> order_id and order_date.\n",
    "* order_items -> order_item_order_id, order_item_subtotal and order_item_product_id.\n",
    "* products -> product_id, product_name.\n",
    "* orders and order_items are in HDFS and products is in local file system.\n",
    "* High level design\n",
    "    * Accept year and month as program argument (along with input and output paths).\n",
    "    * Filter for orders which fall in that month(args).\n",
    "    * Join filtered orders and order_items to get order_item details for that month.\n",
    "    * Get revenue for each product id.\n",
    "    * We need to read products from local file system.\n",
    "    * Convert it into RDD and extract product_id and name.\n",
    "    * Join it with aggregated order_items.\n",
    "    * Get product name and revenue for each product.\n",
    "* Application properties.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>xxxx</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(order_id, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(order_item_order_id, (order_item_product_id, order_item_subtotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#After join (order_id, ((order_item_product_id, order_item_subtotal), 1))\n",
    "\n",
    "(1300, ((191, 199.98), 1))                                                      \n",
    "(1300, ((1014, 199.92), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#After map over join -> for discarding the order_id and 1 from the joined result as we just need product_id as key and sum up the sub_total values in accordance.\n",
    "rec[1][0] ->\n",
    "(order_item_product_id, order_item_subtotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#After reduceByKey we get -> order_item_product_id, product_revenue for that month.\n",
    "#then we need to join product and order_item to get product name and display product_name and product_revenue as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\" style=\"background:#2C3E50;color:white\">Creating Data Frames and Pre Defined Functions</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To work with Data Frames as well as Spark SQL, we need to create object of type Spark Session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>SparkSession available as 'spark'.\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x7fcba9bf3d68>\n",
    "\n",
    ">>> sc\n",
    "<SparkContext master=yarn appName=PySparkShell>\n",
    "    \n",
    ">>> orders = sc.textFile(\"/public/retail_db/orders\")\n",
    ">>> type(orders)\n",
    "<class 'pyspark.rdd.RDD'>\n",
    ">>> orders.first()\n",
    "'1,2013-07-25 00:00:00.0,11599,CLOSED'                                          \n",
    ">>> type(orders.first())\n",
    "<class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD is structure less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DF is nothing but an RDD with structure(schema). As it has structure, we should be able to select certain fields using their names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create DF we can actually read the cols using attributes.\n",
    "creating schema using toDF() fucntion, using this func we can define names for each cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF = sc.read.csv('/public/retail_db/orders').toDF('order_id', 'order_date', 'order_cust_id', 'order_status')\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "AttributeError: 'SparkContext' object has no attribute 'read'\n",
    "\n",
    "#to give names to DF's attributes\n",
    ">>> ordersDF = spark.read.csv('/public/retail_db/orders').toDF('order_id', 'order_date', 'order_cust_id', 'order_status')\n",
    ">>> ordersDF.printSchema()\n",
    "root\n",
    " |-- order_id: string (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- order_cust_id: string (nullable = true)\n",
    " |-- order_status: string (nullable = true)\n",
    "\n",
    ">>> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes/cols in a DF can be referred using names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.select('order_id', 'order_date').show()\n",
    "+--------+--------------------+                                                 \n",
    "|order_id|          order_date|\n",
    "+--------+--------------------+\n",
    "|       1|2013-07-25 00:00:...|\n",
    "|       2|2013-07-25 00:00:...|\n",
    "|       3|2013-07-25 00:00:...|\n",
    "|       4|2013-07-25 00:00:...|\n",
    "|       5|2013-07-25 00:00:...|\n",
    "|       6|2013-07-25 00:00:...|\n",
    "|       7|2013-07-25 00:00:...|\n",
    "|       8|2013-07-25 00:00:...|\n",
    "|       9|2013-07-25 00:00:...|\n",
    "|      10|2013-07-25 00:00:...|\n",
    "|      11|2013-07-25 00:00:...|\n",
    "|      12|2013-07-25 00:00:...|\n",
    "|      13|2013-07-25 00:00:...|\n",
    "|      14|2013-07-25 00:00:...|\n",
    "|      15|2013-07-25 00:00:...|\n",
    "|      16|2013-07-25 00:00:...|\n",
    "|      17|2013-07-25 00:00:...|\n",
    "|      18|2013-07-25 00:00:...|\n",
    "|      19|2013-07-25 00:00:...|\n",
    "|      20|2013-07-25 00:00:...|\n",
    "+--------+--------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we use APIs like select on DF most of the APIs return a DF again. To preview the data in the DF, there's a function called show(), if you apply show().. it wiil show those fields upto 20 records by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF = spark.read.csv('/public/retail_db/orders')\n",
    ">>> ordersDF.show()                                                             \n",
    "+---+--------------------+-----+---------------+\n",
    "|_c0|                 _c1|  _c2|            _c3|\n",
    "+---+--------------------+-----+---------------+\n",
    "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
    "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
    "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
    "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
    "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
    "|  6|2013-07-25 00:00:...| 7130|       COMPLETE|\n",
    "|  7|2013-07-25 00:00:...| 4530|       COMPLETE|\n",
    "|  8|2013-07-25 00:00:...| 2911|     PROCESSING|\n",
    "|  9|2013-07-25 00:00:...| 5657|PENDING_PAYMENT|\n",
    "| 10|2013-07-25 00:00:...| 5648|PENDING_PAYMENT|\n",
    "| 11|2013-07-25 00:00:...|  918| PAYMENT_REVIEW|\n",
    "| 12|2013-07-25 00:00:...| 1837|         CLOSED|\n",
    "| 13|2013-07-25 00:00:...| 9149|PENDING_PAYMENT|\n",
    "| 14|2013-07-25 00:00:...| 9842|     PROCESSING|\n",
    "| 15|2013-07-25 00:00:...| 2568|       COMPLETE|\n",
    "| 16|2013-07-25 00:00:...| 7276|PENDING_PAYMENT|\n",
    "| 17|2013-07-25 00:00:...| 2667|       COMPLETE|\n",
    "| 18|2013-07-25 00:00:...| 1205|         CLOSED|\n",
    "| 19|2013-07-25 00:00:...| 9488|PENDING_PAYMENT|\n",
    "| 20|2013-07-25 00:00:...| 9198|     PROCESSING|\n",
    "+---+--------------------+-----+---------------+\n",
    "only showing top 20 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.show(5)\n",
    "+---+--------------------+-----+---------------+\n",
    "|_c0|                 _c1|  _c2|            _c3|\n",
    "+---+--------------------+-----+---------------+\n",
    "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
    "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
    "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
    "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
    "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
    "+---+--------------------+-----+---------------+\n",
    "only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.limit(5).show()\n",
    "+---+--------------------+-----+---------------+\n",
    "|_c0|                 _c1|  _c2|            _c3|\n",
    "+---+--------------------+-----+---------------+\n",
    "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
    "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
    "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
    "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
    "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
    "+---+--------------------+-----+---------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take(20) converts 20 records to an array\n",
    ">>> ordersDF.select('order_id', 'order_date').take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Row(order_id='1', order_date='2013-07-25 00:00:00.0'), Row(order_id='2', order_date='2013-07-25 00:00:00.0'), Row(order_id='3', order_date='2013-07-25 00:00:00.0'), Row(order_id='4', order_date='2013-07-25 00:00:00.0'), Row(order_id='5', order_date='2013-07-25 00:00:00.0'), Row(order_id='6', order_date='2013-07-25 00:00:00.0'), Row(order_id='7', order_date='2013-07-25 00:00:00.0'), Row(order_id='8', order_date='2013-07-25 00:00:00.0'), Row(order_id='9', order_date='2013-07-25 00:00:00.0'), Row(order_id='10', order_date='2013-07-25 00:00:00.0'), Row(order_id='11', order_date='2013-07-25 00:00:00.0'), Row(order_id='12', order_date='2013-07-25 00:00:00.0'), Row(order_id='13', order_date='2013-07-25 00:00:00.0'), Row(order_id='14', order_date='2013-07-25 00:00:00.0'), Row(order_id='15', order_date='2013-07-25 00:00:00.0'), Row(order_id='16', order_date='2013-07-25 00:00:00.0'), Row(order_id='17', order_date='2013-07-25 00:00:00.0'), Row(order_id='18', order_date='2013-07-25 00:00:00.0'), Row(order_id='19', order_date='2013-07-25 00:00:00.0'), Row(order_id='20', order_date='2013-07-25 00:00:00.0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect() converts all records into an array\n",
    ">>> ordersDF.select('order_id', 'order_date').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop below improves readbility\n",
    ">>> for i in ordersDF.select('order_id', 'order_date').take(20): print(i)\n",
    "... \n",
    "Row(order_id='1', order_date='2013-07-25 00:00:00.0')                           \n",
    "Row(order_id='2', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='3', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='4', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='5', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='6', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='7', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='8', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='9', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='10', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='11', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='12', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='13', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='14', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='15', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='16', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='17', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='18', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='19', order_date='2013-07-25 00:00:00.0')\n",
    "Row(order_id='20', order_date='2013-07-25 00:00:00.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe will give lot more details of your schema than previewing \n",
    "# or priniting the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.describe()\n",
    "DataFrame[summary: string, order_id: string, order_date: string, order_cust_id: string, order_status: string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.describe().show()\n",
    "+-------+------------------+--------------------+-----------------+---------------+\n",
    "|summary|          order_id|          order_date|    order_cust_id|   order_status|\n",
    "+-------+------------------+--------------------+-----------------+---------------+\n",
    "|  count|             68883|               68883|            68883|          68883|\n",
    "|   mean|           34442.0|                null|6216.571098819738|           null|\n",
    "| stddev|19884.953633337947|                null|3586.205241263963|           null|\n",
    "|    min|                 1|2013-07-25 00:00:...|                1|       CANCELED|\n",
    "|    max|              9999|2014-07-24 00:00:...|             9999|SUSPECTED_FRAUD|\n",
    "order+-------+------------------+--------------------+-----------------+---------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.count()\n",
    "68883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once DF is created it can be processed using two approaches:\n",
    "* Native DF APIs - eg select(<fields_name>)\n",
    "* Register DF as Temp Table and run queries against it using spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createTempView creates a temp table/view named orders \n",
    "# on orders we can then use sql API of spark object to run sql queries\n",
    "\n",
    ">>> ordersDF.createTempView('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below command also creates a DF and using show() on it we can view the data.\n",
    "\n",
    ">>> spark.sql('select * from orders')\n",
    "\n",
    "DataFrame[order_id: string, order_date: string, order_cust_id: string, order_status: string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> spark.sql('select * from orders').show()\n",
    "+--------+--------------------+-------------+---------------+                   \n",
    "|order_id|          order_date|order_cust_id|   order_status|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "|       1|2013-07-25 00:00:...|        11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|          256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|        12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|         8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|        11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|         7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|         4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|         2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|         5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|         5648|PENDING_PAYMENT|\n",
    "|      11|2013-07-25 00:00:...|          918| PAYMENT_REVIEW|\n",
    "|      12|2013-07-25 00:00:...|         1837|         CLOSED|\n",
    "|      13|2013-07-25 00:00:...|         9149|PENDING_PAYMENT|\n",
    "|      14|2013-07-25 00:00:...|         9842|     PROCESSING|\n",
    "|      15|2013-07-25 00:00:...|         2568|       COMPLETE|\n",
    "|      16|2013-07-25 00:00:...|         7276|PENDING_PAYMENT|\n",
    "|      17|2013-07-25 00:00:...|         2667|       COMPLETE|\n",
    "|      18|2013-07-25 00:00:...|         1205|         CLOSED|\n",
    "|      19|2013-07-25 00:00:...|         9488|PENDING_PAYMENT|\n",
    "|      20|2013-07-25 00:00:...|         9198|     PROCESSING|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>Reading Text Data from Files</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use spark.read.csv or spark.read.text to read text data.\n",
    "* spark.read.csv can be used for files with some separators between data cols. Default field names will be like _c0, _c1 etc.\n",
    "* spark.read.text can be used to read fixed length data where there is no delimiter. Default field name is value.\n",
    "* toDF() function is used to define custom attribute names.\n",
    "* With either of the above functions for reading, data will be represented as string.\n",
    "* spark.read.format() is a generic function to use to read the file.\n",
    "* We can convert data types by using cast function like this -\n",
    "  <code>df.select(df.field.cast(IntegerType()))</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE: </b> <code>toDF()</code> function doesn't change the datatype of the fields. It just changes their names. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>using spark.read.csv() </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE:  </b> Providing schema while using spark.read.csv allows to provide fields' datatypes along with their custom names.<br>\n",
    "<code>>>> ordersDF = spark.read.csv('/public/retail_db/orders', sep=',', schema='order_id int, order_date string, cust_id int, status string')</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF = spark.read.csv('/public/retail_db/orders', sep=',', schema='order_id int, order_date string, cust_id int, status string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.printSchema()\n",
    "root\n",
    " |-- order_id: integer (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- cust_id: integer (nullable = true)\n",
    " |-- status: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF = spark.read.csv('/public/retail_db/orders').toDF('order_id', 'order_date', 'order_cust_id', 'order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF.printSchema()                                                      \n",
    "root\n",
    " |-- order_id: string (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- order_cust_id: string (nullable = true)\n",
    " |-- order_status: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE: </b> Reading Using spark.read.format -><br> If the format is csv, json, orc, parquet, text etc., then load option is to be specified containing path of the file to be read. For reading from jdbc, the load option doesn't need path. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF_usingFormat = spark.read.format('csv').option('sep', ',').schema('order_id int, order_date string, cust_id int, status string').load('/public/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF_usingFormat.printSchema()\n",
    "root\n",
    " |-- order_id: integer (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- cust_id: integer (nullable = true)\n",
    " |-- status: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF_usingFormat.show()\n",
    "+--------+--------------------+-------+---------------+                         \n",
    "|order_id|          order_date|cust_id|         status|\n",
    "+--------+--------------------+-------+---------------+\n",
    "|       1|2013-07-25 00:00:...|  11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|    256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|  12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|   8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|  11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|   7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|   4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|   2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|   5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|   5648|PENDING_PAYMENT|\n",
    "|      11|2013-07-25 00:00:...|    918| PAYMENT_REVIEW|\n",
    "|      12|2013-07-25 00:00:...|   1837|         CLOSED|\n",
    "|      13|2013-07-25 00:00:...|   9149|PENDING_PAYMENT|\n",
    "|      14|2013-07-25 00:00:...|   9842|     PROCESSING|\n",
    "|      15|2013-07-25 00:00:...|   2568|       COMPLETE|\n",
    "|      16|2013-07-25 00:00:...|   7276|PENDING_PAYMENT|\n",
    "|      17|2013-07-25 00:00:...|   2667|       COMPLETE|\n",
    "|      18|2013-07-25 00:00:...|   1205|         CLOSED|\n",
    "|      19|2013-07-25 00:00:...|   9488|PENDING_PAYMENT|\n",
    "|      20|2013-07-25 00:00:...|   9198|     PROCESSING|\n",
    "+--------+--------------------+-------+---------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE: </b> Another way, of specifying the datatype is to use the cast function, as part of select or withColumn and then typecast the fields to their original datatype.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a DF created  as follows :<br>\n",
    "<code>>>> orders = spark.read.csv('/public/retail_db/orders').toDF('ord_id', 'ord_dt', 'cust_id', 'status')</code>\n",
    "Now, if we want to select from this DF (select is an API used to project data from data frame, creating a new data frame), we can specify the datatype of fields using these 2 ways - \n",
    "* passing datatype as string in cast function .cast(\"int\")<br><code>>>> orders.select(orders.ord_id.cast(\"int\"))\n",
    "DataFrame[ord_id: int]</code>\n",
    "* importing types from pyspark.sql.types  such as IntegerType and then oassing them to cast function as below -<br>\n",
    "<code>>>> from pyspark.sql.types import IntegerType, FloatType</code><br> \n",
    "<code>>>> orders.select(orders.ord_id.cast(\"int\"), orders.ord_dt, orders.cust_id.cast(IntegerType()), orders.status)\n",
    "DataFrame[ord_id: int, ord_dt: string, cust_id: int, status: string]\n",
    "</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>NOTE: </b> But the above way is an overkill as we may have numerous fields in a given data frame and we may only want to typecast 1 or 2. <br> An alternative approach is to use a function called <b>withColumn()</b>. It takes 2 arguments - the first is the alias for the function or expression we are going to use and the second is the cast function.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\">The whole code below -  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders = spark.read.csv('/public/retail_db/orders').toDF('order_id', 'order_date', 'customer_id', 'order_status')\n",
    "\n",
    ">>> orders.printSchema()\n",
    "root\n",
    " |-- order_id: string (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- customer_id: string (nullable = true)\n",
    " |-- order_status: string (nullable = true)\n",
    "\n",
    ">>> orders.show(5)\n",
    "+--------+--------------------+-----------+---------------+                     \n",
    "|order_id|          order_date|customer_id|   order_status|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "only showing top 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #now for casting\n",
    "\n",
    ">>> from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersDF = orders.withColumn('order_id', orders.order_id.cast(\"int\")). \\\n",
    "... withColumn('customer_id', orders.customer_id.cast(IntegerType()))\n",
    "\n",
    ">>> ordersDF.printSchema()\n",
    "root\n",
    " |-- order_id: integer (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- customer_id: integer (nullable = true)\n",
    " |-- order_status: string (nullable = true)\n",
    "\n",
    ">>> ordersDF.show(5)\n",
    "+--------+--------------------+-----------+---------------+                     \n",
    "|order_id|          order_date|customer_id|   order_status|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "only showing top 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>using spark.read.text() </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders = spark.read.text('/public/retail_db/orders')\n",
    ">>> orders.printSchema()\n",
    "root\n",
    " |-- value: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.show(5)\n",
    "+--------------------+                                                          \n",
    "|               value|\n",
    "+--------------------+\n",
    "|1,2013-07-25 00:0...|\n",
    "|2,2013-07-25 00:0...|\n",
    "|3,2013-07-25 00:0...|\n",
    "|4,2013-07-25 00:0...|\n",
    "|5,2013-07-25 00:0...|\n",
    "+--------------------+\n",
    "only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # to view whole row in the data\n",
    "... \n",
    ">>> orders.show(truncate=False)\n",
    "+---------------------------------------------+\n",
    "|value                                        |\n",
    "+---------------------------------------------+\n",
    "|1,2013-07-25 00:00:00.0,11599,CLOSED         |\n",
    "|2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT  |\n",
    "|3,2013-07-25 00:00:00.0,12111,COMPLETE       |\n",
    "|4,2013-07-25 00:00:00.0,8827,CLOSED          |\n",
    "|5,2013-07-25 00:00:00.0,11318,COMPLETE       |\n",
    "|6,2013-07-25 00:00:00.0,7130,COMPLETE        |\n",
    "|7,2013-07-25 00:00:00.0,4530,COMPLETE        |\n",
    "|8,2013-07-25 00:00:00.0,2911,PROCESSING      |\n",
    "|9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT |\n",
    "|10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT|\n",
    "|11,2013-07-25 00:00:00.0,918,PAYMENT_REVIEW  |\n",
    "|12,2013-07-25 00:00:00.0,1837,CLOSED         |\n",
    "|13,2013-07-25 00:00:00.0,9149,PENDING_PAYMENT|\n",
    "|14,2013-07-25 00:00:00.0,9842,PROCESSING     |\n",
    "|15,2013-07-25 00:00:00.0,2568,COMPLETE       |\n",
    "|16,2013-07-25 00:00:00.0,7276,PENDING_PAYMENT|\n",
    "|17,2013-07-25 00:00:00.0,2667,COMPLETE       |\n",
    "|18,2013-07-25 00:00:00.0,1205,CLOSED         |\n",
    "|19,2013-07-25 00:00:00.0,9488,PENDING_PAYMENT|\n",
    "|20,2013-07-25 00:00:00.0,9198,PROCESSING     |\n",
    "+---------------------------------------------+\n",
    "only showing top 20 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.show(5, truncate=False)\n",
    "+-------------------------------------------+\n",
    "|value                                      |\n",
    "+-------------------------------------------+\n",
    "|1,2013-07-25 00:00:00.0,11599,CLOSED       |\n",
    "|2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT|\n",
    "|3,2013-07-25 00:00:00.0,12111,COMPLETE     |\n",
    "|4,2013-07-25 00:00:00.0,8827,CLOSED        |\n",
    "|5,2013-07-25 00:00:00.0,11318,COMPLETE     |\n",
    "+-------------------------------------------+\n",
    "only showing top 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
