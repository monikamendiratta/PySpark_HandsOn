{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\" style=\"background:#2C3E50;color:white\">Compression Algorithms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b>General Guidelines</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You need to balance the processing capacity required to compress and uncompress the data, the disk IO required to read and write the data, and the network bandwidth required to send the data across the network. The correct balance of these factors depends upon the characteristics of your cluster and your data, as well as your usage patterns.\n",
    "* Compression is not recommended if your data is already compressed (such as images in JPEG format). In fact, the resulting file can actually be larger than the original.\n",
    "* GZIP compression uses more CPU resources than Snappy or LZO, but provides a higher compression ratio. GZip is often a good choice for cold data, which is accessed infrequently. Snappy or LZO are a better choice for hot data, which is accessed frequently.\n",
    "* BZip2 can also produce more compression than GZip for some types of files, at the cost of some speed when compressing and decompressing. HBase does not support BZip2 compression.\n",
    "* Snappy often performs better than LZO. It is worth running tests to see if you detect a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of the Standard compression algorithms are -\n",
    "    - gzip\n",
    "    - snappy\n",
    "    - lzo\n",
    "    - bzip2\n",
    "* Some algorithms are splittable while others are non-splittable \n",
    "    - Splittable generate Part compressed files.\n",
    "* Most of the algorithms have bot native as well as java implementations (except for bzip2 - which has only java implementation).\n",
    "* Native implementations are relatively faster than java implementations.\n",
    "* Not only final output but intermediate data can also be compressed in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>Compression Formats / Types</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align='left'><th>Summary of Compression Formats</th>\n",
    "<tr><td>Compression Format</td><td>Tool</td><td>Algorithm</td><td>Filename Extension</td><td>Splittable?</td></tr>\n",
    "<tr><td>Deflate[a]</td><td>N/A</td><td>DEFLATE</td><td>.deflate</td><td>No</td></tr>\n",
    "<tr><td>gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>No</td></tr>\n",
    "<tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>Yes</td></tr>\n",
    "<tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>No[b]</td></tr>\n",
    "<tr><td>LZ4</td><td>N/A</td><td>LZ4</td><td>.lz4</td><td>No</td></tr>    \n",
    "<tr><td>Snappy</td><td>N/A</td><td>Snappy</td><td>.snappy</td><td>No</td></tr>\n",
    "<tr><td>[a] DEFLATE is a compression algorithm whose standard implementation is zlib. There is no commonly available command-line tool for producing files in DEFLATE format, as gzip is normally used. (Note - that gzip file format is DEFLATE with extra headers and a footer). The .deflate extension is hadoop convention.</td><td></td><td></td><td></td><td></td></tr>\n",
    "<tr><td>[b] However, LZO files are splittable, if they have been indexed in a preprocessing step.</td><td></td><td></td><td></td><td></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>Compression Reading & Writing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compressing Text / CSV Files -\n",
    "    - Reading - No special action need to be taken as long as we use supported algorithms.\n",
    "    - Writing - \n",
    "        - Can compress to most of the algorithms (bzip2, deflate, uncompressed, lz4, gzip, snappy, none)\n",
    "        - Use option on spark.write before csv -<br>\n",
    "        <code>df.write.option(\"codec\", \"gzip\").csv(\"<PATH>\")</code>\n",
    "        - Also, option with compression works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compressing Json Files -\n",
    "    - Reading - No special action need to be taken as long as we use supported algorithms.\n",
    "    - Writing - \n",
    "        - Can compress to most of the algorithms (bzip2, deflate, uncompressed, lz4, gzip, snappy, none)\n",
    "        - Use option with compression -<br>\n",
    "        <code>option('compression', 'gzip')</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compressing orc Files -\n",
    "    - Reading - No special action need to be taken as long as we use supported algorithms.\n",
    "    - Writing - \n",
    "        - Default - Snappy\n",
    "        - Supported codecs - none, uncompressed, snappy, zlib, lzo.\n",
    "    - <code>spark.sql.orc.compression.codec</code> - Sets the compression codec used when writing ORC files. If either compression or orc.compress is specified in the table-specific options/properties, the precedence would be compression, orc.compress, spark.sql.orc.compression.codec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compressing parquet Files -\n",
    "    - Reading - No special action need to be taken as long as we use supported algorithms.\n",
    "    - Writing - \n",
    "        - Default - Snappy\n",
    "        - Supported codecs - none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd.\n",
    "        - Set <code>spark.sql.parquet.compression.codec</code> to appropriate algorithm.\n",
    "        - <code>spark.sql.parquet.compression.codec</code> - Sets the compression codec used when writing Parquet files. If either compression or parquet.compression is specified in the table-specific options/properties, the precedence would be compression, parquet.compression, spark.sql.parquet.compression.codec.\n",
    "        - <b>compression</b> – compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, uncompressed, snappy, gzip, lzo, brotli, lz4, and zstd). This will override spark.sql.parquet.compression.codec. If None is set, it uses the value specified in spark.sql.parquet.compression.codec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compressing avro Files -\n",
    "    - Reading - No special action need to be taken as long as we use supported algorithms.\n",
    "    - Writing - \n",
    "        - Default - Snappy\n",
    "        - Supported codecs: uncompressed, deflate, snappy, bzip2 and xz.\n",
    "        - Set <code>spark.sql.avro.compression.codec</code> to appropriate algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>CompressionExamples using different file formats</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Loading pyspark using command below</b> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --master yarn --conf spark.ui.port=21117 --packages com.databricks:spark-avro_2.11:4.0.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SPARK_MAJOR_VERSION is set to 2, using Spark2\n",
    "Python 3.6.8 (default, Aug  7 2019, 17:28:10) \n",
    "[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Ivy Default Cache set to: /home/monahadoop/.ivy2/cache\n",
    "The jars for the packages stored in: /home/monahadoop/.ivy2/jars\n",
    ":: loading settings :: url = jar:file:/usr/hdp/2.6.5.0-292/spark2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
    "com.databricks#spark-avro_2.11 added as a dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('orders_testing'). \\\n",
    "    master('local'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> ordersCSV = spark.read. \\\n",
    "                    format('csv'). \\\n",
    "                    schema('order_id int, order_date string, order_cust_id int, order_status string'). \\\n",
    "                    load('/Users/monikamendiratta/data/retail_db/orders/part-00000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_cust_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    ">>> ordersCSV.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+---------------+\n",
      "|order_id|          order_date|order_cust_id|   order_status|\n",
      "+--------+--------------------+-------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|        11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|          256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|        12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|         8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|        11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|         7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|         4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|         2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|         5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|         5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|          918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|         1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|         9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|         9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|         2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|         7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|         2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|         1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|         9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|         9198|     PROCESSING|\n",
      "+--------+--------------------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersCSV.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- order_id: integer (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- order_cust_id: integer (nullable = true)\n",
    " |-- order_status: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders = ordersCSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab -</b> Creating Directories for better clarity and visibilty </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/csv\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/avro\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/json\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/text\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/parquet\n",
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 6 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:20 /user/monahadoop/spark_compressed/avro\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:19 /user/monahadoop/spark_compressed/csv\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:20 /user/monahadoop/spark_compressed/json\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:21 /user/monahadoop/spark_compressed/orc\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:20 /user/monahadoop/spark_compressed/parquet\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:20 /user/monahadoop/spark_compressed/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> CSV</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Writing/ Saving to .csv file with gzip compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.\\\n",
    "...     format('csv'). \\\n",
    "...     option('codec', 'gzip').\\\n",
    "...     save('/user/monahadoop/spark_compressed/csv/orders_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Viewing.csv file with gzip compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/spark_compressed/csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 1 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:28 /user/monahadoop/spark_compressed/csv/orders_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls  /user/monahadoop/spark_compressed/csv/orders_csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:28 /user/monahadoop/spark_compressed/csv/orders_csv/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs     471106 2020-07-15 19:28 /user/monahadoop/spark_compressed/csv/orders_csv/part-00000-f2004033-b139-47b1-b55d-b566a4aea617-c000.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -h /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 1 items\n",
    "-rw-r--r--   2 hdfs hdfs      2.9 M 2020-07-14 01:35 /public/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -h /user/monahadoop/spark_compressed/csv/orders_csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:28 /user/monahadoop/spark_compressed/csv/orders_csv/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs    460.1 K 2020-07-15 19:28 /user/monahadoop/spark_compressed/csv/orders_csv/part-00000-f2004033-b139-47b1-b55d-b566a4aea617-c000.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> JSON</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Writing/ Saving to .json file with gzip compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.\\\n",
    "...     format('json'). \\\n",
    "...     option('codec', 'gzip'). \\\n",
    "...     save('/user/monahadoop/spark_compressed/json/orders_json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Viewing .json file with gzip compression</b> - didn't get compressed as in above write command, in option 'codec was used instead of compression.' </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed/json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 1 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:34 /user/monahadoop/spark_compressed/json/orders_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed/json/orders_json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:34 /user/monahadoop/spark_compressed/json/orders_json/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs    7201807 2020-07-15 19:34 /user/monahadoop/spark_compressed/json/orders_json/part-00000-769e04ce-50ff-463b-9a22-fa04d78e68a7-c000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -rm -R /user/monahadoop/spark_compressed/json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "20/07/15 19:37:56 INFO fs.TrashPolicyDefault: Moved: 'hdfs://nn01.itversity.com:8020/user/monahadoop/spark_compressed/json' to trash at: hdfs://nn01.itversity.com:8020/user/monahadoop/.Trash/Current/user/monahadoop/spark_compressed/json1594856276119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_compressed/json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Writing/ Saving to .json file with gzip compression - </b>Again with correct command containg 'compression' in option instead of 'codec' </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.option('compression', 'gzip').json('/user/monahadoop/spark_compressed/json/orders_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Viewing .json file with gzip compression</b> - compressed this time </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed/json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 1 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:39 /user/monahadoop/spark_compressed/json/orders_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed/json/orders_json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:39 /user/monahadoop/spark_compressed/json/orders_json/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs     577284 2020-07-15 19:39 /user/monahadoop/spark_compressed/json/orders_json/part-00000-feee6f49-ede7-4c7f-b78e-9a595b6198e0-c000.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -h /user/monahadoop/spark_compressed/json/orders_json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:39 /user/monahadoop/spark_compressed/json/orders_json/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs    563.8 K 2020-07-15 19:39 /user/monahadoop/spark_compressed/json/orders_json/part-00000-feee6f49-ede7-4c7f-b78e-9a595b6198e0-c000.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Writing/ Saving to .json file with gzip compression - </b>with mode='overwrite' </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.option('compression', 'gzip').json('/user/monahadoop/spark_compressed/json/orders_json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> PARQUET</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Writing/ Saving to .parquet file with gzip compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #setting spark property for parquet file compression \n",
    "\n",
    ">>> spark.conf.set('spark.sql.parquet.compression.codec', 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # writing without compression\n",
    "\n",
    ">>> orders.write.parquet('/user/monahadoop/spark_compressed/parquet/orders_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.\\                                                              \n",
    "...     format('parquet').\\\n",
    "...     option('compression', 'gzip'). \\\n",
    "...     save('/user/monahadoop/spark_compressed/parquet/orders_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.\\\n",
    "...     format('parquet').\\\n",
    "...     option('compression', 'gzip'). \\\n",
    "...     save('/user/monahadoop/spark_compressed/parquet/orders_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Viewing .parquet file with gzip compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed/parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 1 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:51 /user/monahadoop/spark_compressed/parquet/orders_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls /user/monahadoop/spark_compressed/parquet/orders_parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:51 /user/monahadoop/spark_compressed/parquet/orders_parquet/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs     276766 2020-07-15 19:51 /user/monahadoop/spark_compressed/parquet/orders_parquet/part-00000-fa82e1cc-2974-4ad5-a615-5037de05e104-c000.gz.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -tail /user/monahadoop/spark_compressed/parquet/orders_parquet/part-00000-fa82e1cc-2974-4ad5-a615-5037de05e104-c000.gz.parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ԃ??*??.❫ل 5iW?v?m????a??z?\n",
    "                          ???:????t??dK??R?W6c?8l?;?ۺ?ՙAW?v%??L?JG?gj?3?????D?Ifj?a?\n",
    "                                                                                    $??y??_??Ue'?$??Nl?\\H\n",
    "  spark_schemorder_id\n",
    "                     %\n",
    "order_cust_id\n",
    "             %\n",
    "              order_status%?order_id???!??\n",
    "&??                                       <\n",
    "\n",
    "   5\n",
    "order_date????G&??\n",
    "                  <2014-07-24 00:00:00.02013-07-25 00:00:00.0,&??\n",
    "order_cust_id?????&??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -h /user/monahadoop/spark_compressed/parquet/orders_parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 19:56 /user/monahadoop/spark_compressed/parquet/orders_parquet/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs    270.3 K 2020-07-15 19:56 /user/monahadoop/spark_compressed/parquet/orders_parquet/part-00000-dbcee5c9-1fb6-4755-80bf-b7ddb12f171c-c000.gz.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> AVRO</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Writing/ Saving to .parquet file with snappy compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #setting spark property for avro file compression \n",
    "\n",
    ">>> spark.conf.set('spark.sql.avro.compression.codec', 'snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> orders.write.\\\n",
    "...     format('com.databricks.spark.avro'). \\\n",
    "...     save('/user/monahadoop/spark_compressed/avro/orders_avro', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b> Viewing .avro file with snappy compression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/spark_compressed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 6 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 20:03 /user/monahadoop/spark_compressed/avro\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:56 /user/monahadoop/spark_compressed/parquet\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:44 /user/monahadoop/spark_compressed/json\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:28 /user/monahadoop/spark_compressed/csv\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:27 /user/monahadoop/spark_compressed/orc\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-15 19:26 /user/monahadoop/spark_compressed/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/spark_compressed/avro/orders_avro"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-15 20:03 /user/monahadoop/spark_compressed/avro/orders_avro/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs     728640 2020-07-15 20:03 /user/monahadoop/spark_compressed/avro/orders_avro/part-00000-0f8e50b8-c39a-438d-b418-76e05db9018e-c000.avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> Reading AVRO & PARQUET Files</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Reading compressed .avro file - </b> To show reading needs no special action.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #reading avro compressed file\n",
    "\n",
    ">>> spark.read. \\\n",
    "...     format('com.databricks.spark.avro'). \\\n",
    "...     load('/user/monahadoop/spark_compressed/avro/orders_avro').\\\n",
    "...     show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------+--------------------+-------------+---------------+\n",
    "|order_id|          order_date|order_cust_id|   order_status|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "|       1|2013-07-25 00:00:...|        11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|          256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|        12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|         8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|        11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|         7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|         4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|         2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|         5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|         5648|PENDING_PAYMENT|\n",
    "|      11|2013-07-25 00:00:...|          918| PAYMENT_REVIEW|\n",
    "|      12|2013-07-25 00:00:...|         1837|         CLOSED|\n",
    "|      13|2013-07-25 00:00:...|         9149|PENDING_PAYMENT|\n",
    "|      14|2013-07-25 00:00:...|         9842|     PROCESSING|\n",
    "|      15|2013-07-25 00:00:...|         2568|       COMPLETE|\n",
    "|      16|2013-07-25 00:00:...|         7276|PENDING_PAYMENT|\n",
    "|      17|2013-07-25 00:00:...|         2667|       COMPLETE|\n",
    "|      18|2013-07-25 00:00:...|         1205|         CLOSED|\n",
    "|      19|2013-07-25 00:00:...|         9488|PENDING_PAYMENT|\n",
    "|      20|2013-07-25 00:00:...|         9198|     PROCESSING|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Reading uncompressed .avro file</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>HDFS commands executed on other terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_uncompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -mkdir /user/monahadoop/spark_uncompressed/avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>on pyspark terminal tab</b> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> spark.conf.set('spark.sql.avro.compression.codec', 'uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #first creating an uncompressed demo .avro file\n",
    "\n",
    ">>> orders.write. \\\n",
    "...     format('com.databricks.spark.avro').\\\n",
    "...     save('/user/monahadoop/spark_uncompressed/avro/orders_avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #reading uncompressed avro file\n",
    "\n",
    ">>> spark.read.\\\n",
    "...     format('com.databricks.spark.avro').\\\n",
    "...     load('/user/monahadoop/spark_uncompressed/avro/orders_avro').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------+--------------------+-------------+---------------+\n",
    "|order_id|          order_date|order_cust_id|   order_status|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "|       1|2013-07-25 00:00:...|        11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|          256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|        12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|         8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|        11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|         7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|         4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|         2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|         5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|         5648|PENDING_PAYMENT|\n",
    "|      11|2013-07-25 00:00:...|          918| PAYMENT_REVIEW|\n",
    "|      12|2013-07-25 00:00:...|         1837|         CLOSED|\n",
    "|      13|2013-07-25 00:00:...|         9149|PENDING_PAYMENT|\n",
    "|      14|2013-07-25 00:00:...|         9842|     PROCESSING|\n",
    "|      15|2013-07-25 00:00:...|         2568|       COMPLETE|\n",
    "|      16|2013-07-25 00:00:...|         7276|PENDING_PAYMENT|\n",
    "|      17|2013-07-25 00:00:...|         2667|       COMPLETE|\n",
    "|      18|2013-07-25 00:00:...|         1205|         CLOSED|\n",
    "|      19|2013-07-25 00:00:...|         9488|PENDING_PAYMENT|\n",
    "|      20|2013-07-25 00:00:...|         9198|     PROCESSING|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> df = spark.read.\\\n",
    "...     format('com.databricks.spark.avro').\\\n",
    "...     load('/user/monahadoop/spark_uncompressed/avro/orders_avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------+--------------------+-------------+---------------+\n",
    "|order_id|          order_date|order_cust_id|   order_status|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "|       1|2013-07-25 00:00:...|        11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|          256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|        12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|         8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|        11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|         7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|         4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|         2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|         5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|         5648|PENDING_PAYMENT|\n",
    "|      11|2013-07-25 00:00:...|          918| PAYMENT_REVIEW|\n",
    "|      12|2013-07-25 00:00:...|         1837|         CLOSED|\n",
    "|      13|2013-07-25 00:00:...|         9149|PENDING_PAYMENT|\n",
    "|      14|2013-07-25 00:00:...|         9842|     PROCESSING|\n",
    "|      15|2013-07-25 00:00:...|         2568|       COMPLETE|\n",
    "|      16|2013-07-25 00:00:...|         7276|PENDING_PAYMENT|\n",
    "|      17|2013-07-25 00:00:...|         2667|       COMPLETE|\n",
    "|      18|2013-07-25 00:00:...|         1205|         CLOSED|\n",
    "|      19|2013-07-25 00:00:...|         9488|PENDING_PAYMENT|\n",
    "|      20|2013-07-25 00:00:...|         9198|     PROCESSING|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- order_id: integer (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- order_cust_id: integer (nullable = true)\n",
    " |-- order_status: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>Reading compressed .parquet file - </b> To show reading needs no special action.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> spark.read.\\\n",
    "...     format('parquet'). \\\n",
    "...     load('/user/monahadoop/spark_compressed/parquet/orders_parquet').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------+--------------------+-------------+---------------+\n",
    "|order_id|          order_date|order_cust_id|   order_status|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "|       1|2013-07-25 00:00:...|        11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|          256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|        12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|         8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|        11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|         7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|         4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|         2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|         5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|         5648|PENDING_PAYMENT|\n",
    "|      11|2013-07-25 00:00:...|          918| PAYMENT_REVIEW|\n",
    "|      12|2013-07-25 00:00:...|         1837|         CLOSED|\n",
    "|      13|2013-07-25 00:00:...|         9149|PENDING_PAYMENT|\n",
    "|      14|2013-07-25 00:00:...|         9842|     PROCESSING|\n",
    "|      15|2013-07-25 00:00:...|         2568|       COMPLETE|\n",
    "|      16|2013-07-25 00:00:...|         7276|PENDING_PAYMENT|\n",
    "|      17|2013-07-25 00:00:...|         2667|       COMPLETE|\n",
    "|      18|2013-07-25 00:00:...|         1205|         CLOSED|\n",
    "|      19|2013-07-25 00:00:...|         9488|PENDING_PAYMENT|\n",
    "|      20|2013-07-25 00:00:...|         9198|     PROCESSING|\n",
    "+--------+--------------------+-------------+---------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> spark.read.\\\n",
    "...     format('parquet'). \\\n",
    "...     load('/user/monahadoop/spark_compressed/parquet/orders_parquet').printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- order_id: integer (nullable = true)\n",
    " |-- order_date: string (nullable = true)\n",
    " |-- order_cust_id: integer (nullable = true)\n",
    " |-- order_status: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AED6F1\"><b>Criteria and Tips</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the ones with native implementation.\n",
    "* Most of the compression algorithms, which have native implementation are non-splittable - which means irrespective of the file size, it is processed by one task only.\n",
    "* To work-around this non-splittable limitation of one task per file (while writing), we need to make sure data is saved in multiple files of manageable size.\n",
    "* Some of the file formats such as parquet, orc etc. are compressed by default. It is better to use default compression (for example parquet is compressed using snappy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#F1C40F\"><b> Example demonstrating Splittable vs Non-Splittable Compression</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking <b>/public/yelp-dataset/yelp_review.csv</b> original uncompressed file for demonstrating.\n",
    "\n",
    "Steps for each -\n",
    "\n",
    "* uncompressed file at this HDFS location<br>\n",
    "    **/public/yelp-dataset/yelp_review.csv**\n",
    "    - check it's number of blocks and files on HDFS using hdfs fsck command.\n",
    "    - This should display more than 24 blocks as file size is 3.8 MB (approx) **[29 blocks]**\n",
    "    - create an RDD for this file and do a count on that RDD, this count runs executors and we can see that it takes more than 24 tasks to complete the count.<br><br>\n",
    "    \n",
    "* compress this file at some other HDFS location<br>\n",
    "    **/user/monahadoop/spark_compressed/csv**\n",
    "    - check it's number of blocks and files on HDFS using hdfs fsck command.\n",
    "    - This should display more than 24 blocks, as it will create split .gz files. **[29 blocks]**\n",
    "    - create an RDD for this file and do a count on that RDD, this count runs executors and we can see that it takes 12 tasks to complete the count.<br><br>\n",
    "    \n",
    "* compress deliberately such that only 1 one non splittable compressed file is there at some other HDFS location. (using coalesce)<br>\n",
    "    **/user/monahadoop/spark_compressed**\n",
    "    - check it's number of blocks and files on HDFS using hdfs fsck command.\n",
    "    - This should display 1 block if compressed in non splittable manner.\n",
    "    - This instead displayed 19 blocks as file size is 1.6 MB (approx) when compressed and this depicts that the compressed file is distributed across 19 blocks with 1 .snappy.parquet file. **[19 blocks]**\n",
    "    - create an RDD for this file and do a count on that RDD, this count runs executors and we can see that it takes 1 tasks to complete the count since it is not splitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\">uncompressed file at this HDFS location<br>\n",
    "    <b>/public/yelp-dataset/yelp_review.csv</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs fsck /public/yelp-dataset/yelp_review.csv -files -blocks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Connecting to namenode via http://172.16.1.101:50070/fsck?ugi=monahadoop&files=1&blocks=1&path=%2Fpublic%2Fyelp-dataset%2Fyelp_review.csv\n",
    "FSCK started by monahadoop (auth:SIMPLE) from /172.16.1.113 for path /public/yelp-dataset/yelp_review.csv at Thu Jul 16 22:12:12 EDT 2020\n",
    "/public/yelp-dataset/yelp_review.csv 3791120545 bytes, 29 block(s):  OK\n",
    "0. BP-292116404-172.16.1.101-1479167821718:blk_1084958116_11226968 len=134217728 repl=2\n",
    "1. BP-292116404-172.16.1.101-1479167821718:blk_1084958117_11226969 len=134217728 repl=2\n",
    "2. BP-292116404-172.16.1.101-1479167821718:blk_1084958118_11226970 len=134217728 repl=2\n",
    "3. BP-292116404-172.16.1.101-1479167821718:blk_1084958119_11226971 len=134217728 repl=2\n",
    "4. BP-292116404-172.16.1.101-1479167821718:blk_1084958120_11226972 len=134217728 repl=2\n",
    "5. BP-292116404-172.16.1.101-1479167821718:blk_1084958121_11226973 len=134217728 repl=2\n",
    "6. BP-292116404-172.16.1.101-1479167821718:blk_1084958122_11226974 len=134217728 repl=2\n",
    "7. BP-292116404-172.16.1.101-1479167821718:blk_1084958123_11226975 len=134217728 repl=2\n",
    "8. BP-292116404-172.16.1.101-1479167821718:blk_1084958125_11226977 len=134217728 repl=2\n",
    "9. BP-292116404-172.16.1.101-1479167821718:blk_1084958126_11226978 len=134217728 repl=2\n",
    "10. BP-292116404-172.16.1.101-1479167821718:blk_1084958127_11226979 len=134217728 repl=2\n",
    "11. BP-292116404-172.16.1.101-1479167821718:blk_1084958128_11226980 len=134217728 repl=2\n",
    "12. BP-292116404-172.16.1.101-1479167821718:blk_1084958129_11226981 len=134217728 repl=2\n",
    "13. BP-292116404-172.16.1.101-1479167821718:blk_1084958130_11226982 len=134217728 repl=2\n",
    "14. BP-292116404-172.16.1.101-1479167821718:blk_1084958131_11226983 len=134217728 repl=2\n",
    "15. BP-292116404-172.16.1.101-1479167821718:blk_1084958132_11226984 len=134217728 repl=2\n",
    "16. BP-292116404-172.16.1.101-1479167821718:blk_1084958133_11226985 len=134217728 repl=2\n",
    "17. BP-292116404-172.16.1.101-1479167821718:blk_1084958134_11226986 len=134217728 repl=2\n",
    "18. BP-292116404-172.16.1.101-1479167821718:blk_1084958135_11226987 len=134217728 repl=2\n",
    "19. BP-292116404-172.16.1.101-1479167821718:blk_1084958136_11226988 len=134217728 repl=2\n",
    "20. BP-292116404-172.16.1.101-1479167821718:blk_1084958137_11226989 len=134217728 repl=2\n",
    "21. BP-292116404-172.16.1.101-1479167821718:blk_1084958138_11226990 len=134217728 repl=2\n",
    "22. BP-292116404-172.16.1.101-1479167821718:blk_1084958139_11226991 len=134217728 repl=2\n",
    "23. BP-292116404-172.16.1.101-1479167821718:blk_1084958140_11226992 len=134217728 repl=2\n",
    "24. BP-292116404-172.16.1.101-1479167821718:blk_1084958141_11226993 len=134217728 repl=2\n",
    "25. BP-292116404-172.16.1.101-1479167821718:blk_1084958142_11226994 len=134217728 repl=2\n",
    "26. BP-292116404-172.16.1.101-1479167821718:blk_1084958143_11226995 len=134217728 repl=2\n",
    "27. BP-292116404-172.16.1.101-1479167821718:blk_1084958144_11226996 len=134217728 repl=2\n",
    "28. BP-292116404-172.16.1.101-1479167821718:blk_1084958145_11226997 len=33024161 repl=2\n",
    "\n",
    "Status: HEALTHY\n",
    " Total size:\t3791120545 B\n",
    " Total dirs:\t0\n",
    " Total files:\t1\n",
    " Total symlinks:\t\t0\n",
    " Total blocks (validated):\t29 (avg. block size 130728294 B)\n",
    " Minimally replicated blocks:\t29 (100.0 %)\n",
    " Over-replicated blocks:\t0 (0.0 %)\n",
    " Under-replicated blocks:\t0 (0.0 %)\n",
    " Mis-replicated blocks:\t\t0 (0.0 %)\n",
    " Default replication factor:\t2\n",
    " Average block replication:\t2.0\n",
    " Corrupt blocks:\t\t0\n",
    " Missing replicas:\t\t0 (0.0 %)\n",
    " Number of data-nodes:\t\t5\n",
    " Number of racks:\t\t1\n",
    "FSCK ended at Thu Jul 16 22:12:12 EDT 2020 in 0 milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating RDD and counting -> this took 29 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelp_rvw = sc.textFile('/public/yelp-dataset/yelp_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelp_rvw.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17746490 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\">compress this file at some other HDFS location<br>\n",
    "    <b>/user/monahadoop/spark_compressed/csv</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelpdf = spark.read. \\\n",
    "...             format('csv'). \\\n",
    "...             load('/public/yelp-dataset/yelp_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normally_compressed\n",
    "\n",
    ">>> yelpdf.write.\\\n",
    "...     option('codec', 'gzip'). \\\n",
    "...     format('csv').\\\n",
    "...     save('/user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 30 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs   55362735 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00021-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55392335 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00013-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55246699 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00012-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55376280 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00008-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55554536 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00005-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55323055 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00000-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55426428 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00026-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55281611 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00025-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55339293 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00024-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55335581 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00022-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55276087 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00023-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55367625 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00019-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55403842 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00027-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55302293 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00020-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   13740884 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00028-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55399644 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00016-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55364992 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00010-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55408407 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00015-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55361267 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00011-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55320475 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00017-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55314441 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00018-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55453488 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00003-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55451191 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00006-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55445277 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00002-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55263666 2020-07-16 22:21 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00014-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55357323 2020-07-16 22:20 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00009-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55454641 2020-07-16 22:20 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00007-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55604060 2020-07-16 22:20 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00004-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz\n",
    "-rw-r--r--   2 monahadoop hdfs   55259374 2020-07-16 22:20 /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed/part-00001-3dd0b129-67a9-426e-bc93-913bbcb76db4-c000.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs fsck /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Connecting to namenode via http://172.16.1.101:50070/fsck?ugi=monahadoop&path=%2Fuser%2Fmonahadoop%2Fspark_compressed%2Fcsv%2Fyelp_review_csv%2Fnormal_compressed\n",
    "FSCK started by monahadoop (auth:SIMPLE) from /172.16.1.113 for path /user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed at Thu Jul 16 22:26:19 EDT 2020\n",
    "..............................Status: HEALTHY\n",
    " Total size:\t1564187530 B\n",
    " Total dirs:\t1\n",
    " Total files:\t30\n",
    " Total symlinks:\t\t0\n",
    " Total blocks (validated):\t29 (avg. block size 53937501 B)\n",
    " Minimally replicated blocks:\t29 (100.0 %)\n",
    " Over-replicated blocks:\t0 (0.0 %)\n",
    " Under-replicated blocks:\t0 (0.0 %)\n",
    " Mis-replicated blocks:\t\t0 (0.0 %)\n",
    " Default replication factor:\t2\n",
    " Average block replication:\t2.0\n",
    " Corrupt blocks:\t\t0\n",
    " Missing replicas:\t\t0 (0.0 %)\n",
    " Number of data-nodes:\t\t5\n",
    " Number of racks:\t\t1\n",
    "FSCK ended at Thu Jul 16 22:26:19 EDT 2020 in 1 milliseconds\n",
    "The filesystem under path '/user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed' is HEALTHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating RDD and counting -> took 29 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelp_nc = sc.textFile('/user/monahadoop/spark_compressed/csv/yelp_review_csv/normal_compressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelp_nc.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12454738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\">compress deliberately such that only 1 one non splittable compressed file is there at some other HDFS location. (using coalesce)<br>\n",
    "    <b>/user/monahadoop/spark_compressed</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelpdf.\\\n",
    "...     coalesce(1). \\\n",
    "...     write. \\\n",
    "...     option('codec', 'gzip'). \\\n",
    "...     save('/user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created .snappy.parquet file even though gzip compression was given-> No idea Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-16 22:33 /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs 2473989905 2020-07-16 22:33 /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed/part-00000-d79829db-5ab4-483f-86fa-a5b777973aff-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs fsck /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed/part-00000-d79829db-5ab4-483f-86fa-a5b777973aff-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Connecting to namenode via http://172.16.1.101:50070/fsck?ugi=monahadoop&path=%2Fuser%2Fmonahadoop%2Fspark_compressed%2Fcsv%2Fyelp_review_csv%2Fcoalesce_compressed%2Fpart-00000-d79829db-5ab4-483f-86fa-a5b777973aff-c000.snappy.parquet\n",
    "FSCK started by monahadoop (auth:SIMPLE) from /172.16.1.113 for path /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed/part-00000-d79829db-5ab4-483f-86fa-a5b777973aff-c000.snappy.parquet at Thu Jul 16 22:35:30 EDT 2020\n",
    ".Status: HEALTHY\n",
    " Total size:\t2473989905 B\n",
    " Total dirs:\t0\n",
    " Total files:\t1\n",
    " Total symlinks:\t\t0\n",
    " Total blocks (validated):\t19 (avg. block size 130209995 B)\n",
    " Minimally replicated blocks:\t19 (100.0 %)\n",
    " Over-replicated blocks:\t0 (0.0 %)\n",
    " Under-replicated blocks:\t0 (0.0 %)\n",
    " Mis-replicated blocks:\t\t0 (0.0 %)\n",
    " Default replication factor:\t2\n",
    " Average block replication:\t2.0\n",
    " Corrupt blocks:\t\t0\n",
    " Missing replicas:\t\t0 (0.0 %)\n",
    " Number of data-nodes:\t\t5\n",
    " Number of racks:\t\t1\n",
    "FSCK ended at Thu Jul 16 22:35:30 EDT 2020 in 0 milliseconds\n",
    "\n",
    "\n",
    "The filesystem under path '/user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed/part-00000-d79829db-5ab4-483f-86fa-a5b777973aff-c000.snappy.parquet' is HEALTHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  trying to do gzip compression again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelpdf.coalesce(1).write.option('codec', 'gzip'). \\\n",
    "...      save('/user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed_gz', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed_gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it still created .snappy.parquet file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-16 22:41 /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed_gz/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs 2473989905 2020-07-16 22:41 /user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed_gz/part-00000-06919193-b42b-4043-b15f-351f8c94cc89-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating RDD and counting -> took 19 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelp_cc = sc.textFile('/user/monahadoop/spark_compressed/csv/yelp_review_csv/coalesce_compressed/part-00000-d79829db-5ab4-483f-86fa-a5b777973aff-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yelp_cc.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "28620042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time it created .csv.gz with .format('csv') file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> op.coalesce(1).write.option('codec', 'gzip').format('csv').\\\n",
    "...     save('/user/monahadoop/compressed/yelp_review_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/compressed\n",
    "Found 2 items\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-17 02:42 /user/monahadoop/compressed/yelp_review_csv\n",
    "drwxr-xr-x   - monahadoop hdfs          0 2020-07-17 02:26 /user/monahadoop/compressed/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[monahadoop@gw03 ~]$ hdfs dfs -ls -t /user/monahadoop/compressed/yelp_review_csv\n",
    "Found 2 items\n",
    "-rw-r--r--   2 monahadoop hdfs          0 2020-07-17 02:42 /user/monahadoop/compressed/yelp_review_csv/_SUCCESS\n",
    "-rw-r--r--   2 monahadoop hdfs 1564151568 2020-07-17 02:42 /user/monahadoop/compressed/yelp_review_csv/part-00000-97fdbe13-d109-413d-b7db-6aad143e4a62-c000.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background :#d0d5db\"><b>END</b> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
